{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26089e-f1b7-4d31-acd8-b5f1e4f70196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0ba4136-09e5-4f09-ba07-26485995229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable # 자동미분\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03e2361-c0cd-4840-bd6d-c2c0f6349b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      " - pytorch\n",
      " - anaconda\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\user\\anaconda3\\envs\\torch-book\n",
      "\n",
      "  added / updated specs:\n",
      "    - tqdm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    tqdm-4.66.4                |  py312hfc267ef_0         188 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         188 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  tqdm               pkgs/main/win-64::tqdm-4.66.4-py312hfc267ef_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2024.7.4-~ --> pkgs/main/win-64::certifi-2024.7.4-py312haa95532_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install tqdm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070316ce-6a65-4811-9654-28a9224b73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c729ed02-62d3-4a34-9d9c-2e75488e04ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 253 entries, 2019-12-11 to 2020-12-10\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Open       253 non-null    float64\n",
      " 1   High       253 non-null    float64\n",
      " 2   Low        253 non-null    float64\n",
      " 3   Close      253 non-null    float64\n",
      " 4   Adj Close  253 non-null    float64\n",
      " 5   Volume     253 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.8 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/SBUX.csv\", index_col=[\"Date\"], date_format=\"%Y-%m-%d\") # 원하는타입으로 파싱\n",
    "data[\"Volume\"] = data[\"Volume\"].astype(float) # 거래량도 실수로 변환\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49705fc-b29d-4bba-976f-6d4514ab4bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close\n",
      "Date                                                                  \n",
      "2019-12-11   86.260002   86.870003   85.849998   86.589996   84.145752\n",
      "2019-12-12   88.000000   88.889999   87.540001   88.209999   85.720032\n",
      "2019-12-13   88.019997   88.790001   87.580002   88.669998   86.167046\n",
      "2019-12-16   89.139999   89.300003   88.430000   88.779999   86.273941\n",
      "2019-12-17   88.870003   88.970001   87.470001   88.129997   85.642288\n",
      "...                ...         ...         ...         ...         ...\n",
      "2020-12-04  101.349998  102.940002  101.070000  102.279999  101.442787\n",
      "2020-12-07  102.010002  102.220001  100.690002  101.410004  100.579918\n",
      "2020-12-08  100.370003  101.570000  100.010002  101.209999  100.381554\n",
      "2020-12-09  101.940002  102.209999  100.099998  100.400002   99.578186\n",
      "2020-12-10  103.510002  106.089996  102.750000  105.389999  104.527336\n",
      "\n",
      "[253 rows x 5 columns]                 Volume\n",
      "Date                  \n",
      "2019-12-11   4921900.0\n",
      "2019-12-12  10282100.0\n",
      "2019-12-13   6714100.0\n",
      "2019-12-16   6705600.0\n",
      "2019-12-17   7296900.0\n",
      "...                ...\n",
      "2020-12-04   6952700.0\n",
      "2020-12-07   4514800.0\n",
      "2020-12-08   3911300.0\n",
      "2020-12-09   6629900.0\n",
      "2020-12-10  12939200.0\n",
      "\n",
      "[253 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X=data.iloc[:, :-1] # 모든행, 마지막 열을 제외한 모든 열\n",
    "y=data.iloc[:, 5:6] # 모든행, 6번째값만 (인덱스5)\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aca0558-d5b7-4a92-9d40-71c777f70a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_ss = ss.fit_transform(X)\n",
    "y_ms = ms.fit_transform(y)\n",
    "\n",
    "X_train = X_ss[:200, :]\n",
    "X_test = X_ss[200:, :]\n",
    "\n",
    "y_train = y_ms[:200, :]\n",
    "y_test = y_ms[200:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9735a1-50f0-4775-b0ec-4887d326f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_ss, y_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c84dc5b-109e-4d54-b075-01bde703e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5) (53, 5) (200, 1) (53, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d63150-085c-408f-beec-176737056e76",
   "metadata": {},
   "source": [
    "### 파이토치 관련\n",
    "- 데이터를 텐서로 변경\n",
    "- 모델 설계\n",
    "- 모델 생성 후 학습\n",
    "- 예측\n",
    "- 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ab7387a-d2c8-4e0e-9c4d-40ff619aedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7c186f7-198a-4369-89f6-cfecee93f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 5]) torch.Size([200, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 주의\n",
    "X_train_tensors_f = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_f = torch.reshape(X_test_tensors, (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n",
    "\n",
    "# reshape하는 과정이나 이유에 대해서는 아래쪽에서\n",
    "print(X_train_tensors.shape, X_train_tensors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4c7a600-4553-429b-915d-b7231848ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    # 모델을 어떻게 작성해야 하나요?\n",
    "        # 생성자부터 작성 => \"내가 직접 결정해야하는 값 : 하이퍼파라미터\"\n",
    "        # 순전파 작성\n",
    "        # 역전파는 작성하지 않음\n",
    "    \n",
    "    def __init__(self, num_classes, num_layers, hidden_size, input_size,  seq_length): # num_classes는 아웃풋, num_layers, input_size, hidden_size는 인풋\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes # 아웃풋 갯수 \n",
    "        self.input_size = input_size # feature 갯수 (column 몇개로 볼지)\n",
    "        self.hidden_size = hidden_size # 단기기억 메모리수\n",
    "        self.num_layers = num_layers # layer를 몇 층으로 쌓을지\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True # 변수 순서 바꿔주는거 [batch_size, seq, feature]\n",
    "        )\n",
    "\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU() # 활성화함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) # x.size(0) 이건 배치사이즈\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        output, (hn,cn) = self.lstm(x, (h_0, c_0))\n",
    "        hn = hn.view(-1, self.hidden_size) # hn모양을 바꿔줌\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a134b4e3-f4bb-48a9-8564-eca6f94985ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "num_classes = 1\n",
    "\n",
    "# 이 6개가 가장 중요한 하이퍼파라미터\n",
    "\n",
    "model = LSTM(num_classes, input_size, hidden_size, num_layers, X_train_tensors_f.shape[1])\n",
    "\n",
    "criterion = torch.nn.MSELoss() # 회귀니까 loss사용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # learning_rate는 하이퍼파라미터 (설정해야함 -> 지나칠수도, 너무오래걸릴수도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6db4996-7026-44d1-bc9a-83d9745c3b0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 1, got 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensors_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_tensor)\n",
      "Cell \u001b[1;32mIn[29], line 31\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m h_0 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)) \u001b[38;5;66;03m# x.size(0) 이건 배치사이즈\u001b[39;00m\n\u001b[0;32m     29\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m---> 31\u001b[0m output, (hn,cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m hn \u001b[38;5;241m=\u001b[39m hn\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size) \u001b[38;5;66;03m# hn모양을 바꿔줌\u001b[39;00m\n\u001b[0;32m     33\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(hn)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:913\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    910\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m--> 913\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:827\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    823\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    824\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    825\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    826\u001b[0m                        ):\n\u001b[1;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    829\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    831\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 1, got 5"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    outputs = model.forward(X_train_tensors_f)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 ==0:\n",
    "        print(f\"{epoch}:{loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
